{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers pandas nltk rouge-score tqdm sentencepiece"
      ],
      "metadata": {
        "id": "WwF20QK4C-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BartForConditionalGeneration, PegasusForConditionalGeneration, T5ForConditionalGeneration, BartTokenizer, PegasusTokenizer, T5Tokenizer\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel(\"ar_sum_dataset.xlsx\", nrows=5)\n",
        "articles = data['content'][:10].tolist()\n",
        "human_summaries = data['human_summary'][:10].tolist()\n",
        "\n",
        "# Initializing models and tokenizers\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Function for text summarization\n",
        "def generate_summary(model, tokenizer, input_text, max_length=150):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Generate summaries for each model\n",
        "bart_summaries = []\n",
        "\n",
        "for article in tqdm(articles):\n",
        "    # BART\n",
        "    bart_summary = generate_summary(bart_model, bart_tokenizer, article)\n",
        "    bart_summaries.append(bart_summary)\n",
        "\n",
        "# Evaluating ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores_bart = [scorer.score(human_summaries[i], bart_summaries[i]) for i in range(len(human_summaries))]\n",
        "\n",
        "# ROUGE scores\n",
        "rouge_scores = {\n",
        "    \"BART\": {\n",
        "        \"ROUGE-1\": [score['rouge1'].fmeasure for score in scores_bart],\n",
        "        \"ROUGE-2\": [score['rouge2'].fmeasure for score in scores_bart],\n",
        "        \"ROUGE-L\": [score['rougeL'].fmeasure for score in scores_bart]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ROUGE scores in a table\n",
        "rouge_df = pd.DataFrame(rouge_scores)\n",
        "\n",
        "print(\"ROUGE Scores:\")\n",
        "print(rouge_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxRItQC9ExeH",
        "outputId": "55f58346-3fda-498d-9456-c6ba828dc7bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [02:11<00:00, 26.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores:\n",
            "                                                      BART\n",
            "ROUGE-1  [0.2435897435897436, 0.13114754098360656, 0.24...\n",
            "ROUGE-2  [0.14193548387096774, 0.03305785123966942, 0.0...\n",
            "ROUGE-L  [0.17307692307692307, 0.09016393442622951, 0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}